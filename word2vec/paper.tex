\documentclass[twocolumn]{jarticle}
\title{単語分散表現, word2vec}
\author{白坂貴規}
\date{\today}
\setlength{\topmargin}{-2cm}
\setlength{\textheight}{250mm}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath, amssymb, bm}

%表のテンプレ
%\begin{table}[!htbp]
%  \begin{center}
%    \caption{}
%    \begin{tabular}{ccc} \hline
%    \end{tabular}
%    \label{tab:}
%  \end{center}
%\end{table}

%図のテンプレ
%\begin{figure}[!htbp]
%  \begin{center}
%    \includegraphics[width=14cm]{パス}
%    \caption{}
%    \label{fig:}
%  \end{center}
%\end{figure}

\begin{document}
\maketitle

\section{自然言語と単語の分散表現}
コンピュータに自然言語を理解させる手法には三種類あり, シソーラスによる手法, カウントベースの手法, 推論ベースの手法である.

\subsection{シソーラス}
シソーラスとは, (基本的には)類似辞書であり「同じ意味の単語」や「意味の似た単語」が同じグループに分類されている. また, 自然言語処理(NLP)において利用されるシソーラスでは単語間で「上位と下位」「全体と部分」などのより細かいかんれんせいが定義されている場合がある. NLPにおいて最も有名なシソーラスは1985年にプリンストン大学で開発がスタートしたWordNetである.

\subsection{シソーラスの問題点}
時代の変化に対応するのが困難
人の作業コストが高い
単語の細かなニュアンスを表現できない

\section{カウントベースの手法}

\subsection{単語の分散表現}
単語の意味を捉えたベクトル表現を単語の分散表現とよぶ. これは, 単語の意味は周辺の単語によって形成されるという分布仮説によって成り立っている. たとえば「I drink water.」「We drink water.」のようにdrinkの周辺には飲み物が出現する. 周辺の単語を左右何単語まで含めるかをウィンドウサイズとよび, 「You say goodbye and I say hello.」という具体例を用いてコンテキストに含まれる単語の頻度を数える.
% 共起行列の表
これは共起行列と呼ばれ, それぞれの行を参照することにより各単語のベクトルを得ることができる.

\subsection{ベクトル間の類似度}
様々な手法があるが, 単語のベクトル表現の類似度に関しては次式で定義されるコサイン類似度がよく用いられる.
\begin{equation}
  similarity({\bm {x}}, {\bm {y}}) = \frac{{\bm {x}}・{\bm {y}}}{||{\bm x}|| ||{\bm y}||}
\end{equation}

\subsection{単語の分散表現の改良}
　以上の共起行列では単に共起する回数に着目しているので, 「drive」と「car」よりも「the」と「car」の方が強い関連性を持ってしまう. これを改善するために以下で定義する相互情報量(Pointwise Mutual Information)という指標を使う.
\begin{equation}
  PMI(x, y) = \log_{2} \frac{P(x, y)}{P(x)P(y)}
\end{equation}
ここで, ${P(x)}$は${x}$が起こる確率, ${P(y)}$は${y}$が起こる確率, ${P(x, y)}$は${x}$と${y}$が同時に起こる確率を表す. また, PMIはその値が高いほど関連性が高いことを示す. 例えば10000個の単語からなるコーパスで「the」が1000回,「car」が20回,「drive」が10回出現し,「the」と「car」が10回共起し, 「drive」と「car」が5回共起したとする. その時, PMI("the", "car") = 2.32, PMI("car", "drive") = 7.97となり正しく評価できている. また, 実践上では負のPMIは0にする正の相互情報量を用いる.

\subsection{次元削減}
　次元削減には様々な手法があるがここでは特異値分解(Singular Value Decomposition: SVD)を行う. 任意の行列${X}$を, ${U}$, ${S}$, ${V}$の３つに分解する. つまり,
\begin{equation}
  X = USV^T
\end{equation}
ここで${U}$と${V}$は直行行列でありその列ベクトルは互いに直交する. ${S}$は対角行列であり, 対応する軸の重要度を示す特異値が降順に並んでいるので特異値が大きいものに対応する軸のみ残すことで次元削減が可能になる.

\subsection{カウントベースの手法の問題点}
現実的にはコーパスで扱う語彙数は非常に巨大で語彙数が100万をゆうに超えると言われている. その際カウントベースの手法では100万×100万の巨大な行列をつくることになり, n×nの行列に対して${O(n^3)}$の計算量がかかる特異値分解は現実的ではない.

\section{推論ベースの手法}
推論ベースの手法では, 周囲の単語(コンテキスト)が与えられたときにどのような単語が出現するかを推測し, 学習する. その学習の結果として単語の分散表現を得られるというのが推論ベースの手法である.

\subsection{シンプルなword2vec}
単語はone-hotベクトル化することで入力されるニューロンを固定長のベクトルにすることが可能. これをモデルへの入力とするが, continuous bag-of-words(CBOW)と呼ばれるモデルを使う.

%　図3.9 CBOWモデルのネットワーク構造

注意すべきは中間層にあるニューロンは各入力層の全結合による変換後の値が相加平均されていることである. 最終的な出力は各単語のスコアであり, Softmax関数を適用することで単語の出現確率が求められる. 損失関数は, 交差エントロピー誤差を用いる.
最終的に利用する単語の分散表現は, 多くの研究では入力側の重み${W_{in}}$だけを利用する

\subsection{CBOWモデルの順伝播}

\subsection{CBOWモデルの逆伝播}

\subsection{CBOWモデルの確率的表現}
コーパスを${w_1, w_2, \ddots, w_T}$で表現するとする. コンテキストとして${w_{t-1}}$, ${w_{t+1}}$が与えられたときに, ターゲットが${w_t}$となる確率は
\begin{equation}
  P(w_t|w_{t-1}, w_{t+1})
\end{equation}
で表される. これは「${w_{t-1}, w_{t+1}}$が与えられたときに${w_t}$が起こる確率」を表しているのでCBOWモデルを表現しているといえる. 損失関数である交差エントロピーも${L = - \sum_{k} t_k \log y_k }$であり, ${t_k}$がone-hotベクトルの教師ベクトルであることに注意すると
\begin{equation}
  L = - \log P(w_t|w_{t-1}, w_{t+1})
\end{equation}
となる. これは単に確率に対数をとって−１倍したものである. これを一つのサンプルデータからコーパス全体に拡張することで損失関数は以下のようになる.

\begin{equation}
  L = - \frac{1}{T} \sum^{T}_{t=1} \log P(w_t|w_{t-1}, w_{t+1})
\end{equation}
CBOWモデルの学習で行うことは, この損失関数をできる限り小さくすることである. そしてその時の重みパラメータが目的とする単語の分散表現である.


\subsection{skip-gramモデル}
word2vecではCBOWモデルの他にskip-gramモデルが提案されている. CBOWモデルは周囲の単語から中央の単語を予測するのに対し, skip-gramモデルでは中央の単語から周囲の単語を予測する.

%　図3.24 skip-gramモデルのネットワーク構造

CBOWモデルと同様に確率の表記で表現する. まず, 単語の予測は次の確率で表現される.
\begin{equation}
  P(w_{t-1}, w_{t+1}|w_t) = P(w_{t-1}|w_t)P(w_{t+1}|w_t)
\end{equation}
これを交差エントロピー誤差に適用することでskip-gramモデルの損失関数が次のように導ける.
\begin{equation}
  L &=& - \log P(w_{t-1}, w_{t+1}|w_t) \\
    &=& - \log P(w_{t-1}|w_t)P(w_{t+1}|w_t) \\
    &=& - (\log P(w_{t-1}|w_t) + \log P(w_{t+1}|w_t))
\end{equation}
これをコーパス全体に拡張することにより
\begin{equation}
  L = - \frac{1}{T} \sum^{T}_{t=1} (\log P(w_{t-1}|w_t) + \log P(w_{t+1}|w_t))
\end{equation}
が導かれる.
コーパスが大規模になるにつれて低頻出の単語や類推の性能の点においてskip-gramモデルの方が優れている. 一方で, 学習速度の点ではCBOWモデルの方が優秀である.

\section{カウントベース v.s. 推論ベース}
単語の分散表現の更新作業が発生した場合, カウントベースの手法はゼロから計算を行う必要があるのに対し, 推論ベースはパラメータを再学習するときに初期値を以前のものに設定することで効率的に学習が行える.
カウントベースの手法では主に単語の類似性がエンコードされる
推論ベースの手法では単語の類似性に加えて複雑な単語間のパターンも捉えられる(word2vecは「king - man + woman = queen」が解ける.)
しかし, 推論ベースとカウントベースは優劣がつけられれない.


\begin{thebibliography}{9}
  \bibitem{教科書} 瀧雅人. これならわかる深層学習入門. p114-131
  \bibitem{鍋谷} 鍋谷昴一. 単純パーセプトロンの収束定理と限界. http://starpentagon.net/analytics/simple\_perceptron/
\end{thebibliography}
\end{document}
